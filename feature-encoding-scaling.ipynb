{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b48e44ec-54fa-4265-b618-74067e601354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T15:12:55.235859Z",
     "start_time": "2026-01-21T15:12:55.223928Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f714bbbcc101f124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T15:12:55.307699Z",
     "start_time": "2026-01-21T15:12:55.300051Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # for data manipulation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# for preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cada41e-3466-4ce3-86ba-46a855c2e32d",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd85138a-b021-40e1-82c9-5a992c577e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da9543de-4860-4c86-9bd1-5b9999de5ad8",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d348698d-1354-4513-a530-c6c193c12647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Info after cleaning missing values:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45222 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   age              45222 non-null  int64 \n",
      " 1   workclass        45222 non-null  object\n",
      " 2   fnlwgt           45222 non-null  int64 \n",
      " 3   education        45222 non-null  object\n",
      " 4   educational-num  45222 non-null  int64 \n",
      " 5   marital-status   45222 non-null  object\n",
      " 6   occupation       45222 non-null  object\n",
      " 7   relationship     45222 non-null  object\n",
      " 8   race             45222 non-null  object\n",
      " 9   gender           45222 non-null  object\n",
      " 10  capital-gain     45222 non-null  int64 \n",
      " 11  capital-loss     45222 non-null  int64 \n",
      " 12  hours-per-week   45222 non-null  int64 \n",
      " 13  native-country   45222 non-null  object\n",
      " 14  income           45222 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.replace('?', np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(\"Initial Data Info after cleaning missing values:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fb913f8-a6e2-4a4b-b58a-0a4fbc04dec4",
   "metadata": {},
   "source": [
    "1. Identify categorical and numerical features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fa6a91f-d417-41ab-b081-8273c558524d",
   "metadata": {},
   "source": [
    "1. Numerical Features\n",
    "    1.1 age\n",
    "    1.2 fnlwgt (Final weight of the record. Basically interpret as the number of people represented by this row)\n",
    "    1.3 education-num\n",
    "    1.4 capital-gain\n",
    "    1.5 capital-loss\n",
    "    1.6 hours-per-week\n",
    "2. Categorial Features\n",
    "    2.1 workclass\n",
    "    2.2 marital-status\n",
    "    2.3 occupation\n",
    "    2.4 relationship\n",
    "    2.5 race\n",
    "    2.6 gender\n",
    "    2.7 native-country\n",
    "    2.8 education\n",
    "3. Target Label\n",
    "    3.1 Income\n",
    "\n",
    "Total: 14 Features, 1 Target Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb6c4de-01dc-4078-8e28-d875229916ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country'] # education is ordinal but education-num is present\n",
    "target_feature = 'income'\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(target_feature, axis=1) # axis=1 means look for the column, 0 will look for row\n",
    "y = df[target_feature]\n",
    "\n",
    "# Features vs. Target\n",
    "# By \"dropping\" the target feature, you are creating a version of the dataset that contains everything except the answer key.\n",
    "\n",
    "# If you do not drop the target feature from X, your model will have the \"answer\" inside its input data. This is called Data Leakage.\n",
    "\n",
    "# The Result: The model would achieve 100% accuracy during training because it is simply looking at the income column to predict income, but it would be completely useless when trying to predict data where the income is unknown."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc57db0c-357c-4c30-9504-ee89a26a74a2",
   "metadata": {},
   "source": [
    "2. Apply Label Encoding where order exists."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f211f4c0-4848-473b-848e-b4c6f9423dec",
   "metadata": {},
   "source": [
    "Ordinal Features - Categories with natural ranking or order\n",
    "1. Feature - educational-num (education being text)\n",
    "2. Target - income (<=50k can be encoded as 0 and >50k as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ace4e7d-8b08-4031-a45e-476997494580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target Variable Mapping: {'<=50K': np.int64(0), '>50K': np.int64(1)}\n"
     ]
    }
   ],
   "source": [
    "# Target variable 'income' is binary, so LabelEncoder is appropriate\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(\"\\nTarget Variable Mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "# We'll use educational-num instead of text 'education' as it's already encoded ordinally.\n",
    "categorical_features.remove('education') "
   ]
  },
  {
   "cell_type": "raw",
   "id": "89c77449-89c6-417a-a892-312e0241d2d4",
   "metadata": {},
   "source": [
    "3. Apply One-Hot Encoding where order does not exist."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5a2df70-56a8-4fcf-a471-1cf146f7b8fc",
   "metadata": {},
   "source": [
    "This is for nominal feature\n",
    "workclass, marital-status, occupation, relationship, race, gender, and native-country"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69e815a2-b45c-4f39-a366-693bc64d6737",
   "metadata": {},
   "source": [
    "4. Scale numerical features using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b01788a-e9e4-406e-8340-3c7f21625c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of original data (rows, cols): (45222, 14)\n",
      "Shape of processed data (rows, cols): (45222, 89)\n"
     ]
    }
   ],
   "source": [
    "# Use ColumnTransformer to apply different preprocessing steps to different columns\n",
    "\n",
    "# Create the preprocessing pipelines for numerical and categorical data\n",
    "# Ensure categorical columns are NOT in numerical_features list.\n",
    "# Set sparse_output=False in OneHotEncoder to prevent sparse matrix errors.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns as they are (none in this case)\n",
    ")\n",
    "\n",
    "# Apply the preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "print(f\"\\nShape of original data (rows, cols): {X.shape}\")\n",
    "print(f\"Shape of processed data (rows, cols): {X_processed.shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b67814ff-c891-417b-a5d3-6bfc22a459d9",
   "metadata": {},
   "source": [
    "5. Compare model readiness before and after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e50ab70b-f9e1-4846-ab0a-ee2da96b85a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Readiness Comparison (Numerical Features) ---\n",
      "Before Scaling (Raw Data):\n",
      "                           mean            std\n",
      "age                  38.547941      13.217870\n",
      "fnlwgt           189734.734311  105639.195134\n",
      "educational-num      10.118460       2.552881\n",
      "capital-gain       1101.430344    7506.430084\n",
      "capital-loss         88.595418     404.956092\n",
      "hours-per-week       40.938017      12.007508\n",
      "\n",
      "After Scaling (StandardScaler):\n",
      "                  mean  std\n",
      "age              -0.0  1.0\n",
      "fnlwgt            0.0  1.0\n",
      "educational-num   0.0  1.0\n",
      "capital-gain     -0.0  1.0\n",
      "capital-loss      0.0  1.0\n",
      "hours-per-week    0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Readiness Comparison (Numerical Features) ---\")\n",
    "# Before Scaling - Using .agg to avoid KeyError\n",
    "stats_before = df[numerical_features].agg(['mean', 'std']).T\n",
    "print(\"Before Scaling (Raw Data):\\n\", stats_before)\n",
    "\n",
    "X_processed_array = preprocessor.fit_transform(X)\n",
    "\n",
    "# After Scaling\n",
    "# Reconstruct a temporary DF of the scaled numerical part to check stats\n",
    "X_scaled_num = pd.DataFrame(X_processed_array[:, :len(numerical_features)], columns=numerical_features)\n",
    "stats_after = X_scaled_num.agg(['mean', 'std']).T\n",
    "print(\"\\nAfter Scaling (StandardScaler):\\n\", stats_after.round(2)) \n",
    "# Stats should now show Mean ≈ 0 and Std ≈ 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "790e4a2b-d991-45ba-a84f-9d6daeb4eeaa",
   "metadata": {},
   "source": [
    "6. Explain impact of scaling on ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1197fb8f-0454-4f2c-9982-435916dbb883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Impact of Scaling on ML Algorithms ---\n",
      "*   **Distance-based algorithms (KNN, SVM, K-Means)**: Highly affected; scaling ensures all features contribute equally to distance calculations.\n",
      "*   **Gradient Descent-based algorithms (Logistic Regression, Neural Networks)**: Highly affected; scaling leads to faster convergence and numerical stability.\n",
      "*   **Tree-based algorithms (Decision Trees, Random Forests, XGBoost)**: Generally invariant to scaling as they rely on split points and relative order, not magnitude.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Impact of Scaling on ML Algorithms ---\")\n",
    "print(\"*   **Distance-based algorithms (KNN, SVM, K-Means)**: Highly affected; scaling ensures all features contribute equally to distance calculations.\")\n",
    "print(\"*   **Gradient Descent-based algorithms (Logistic Regression, Neural Networks)**: Highly affected; scaling leads to faster convergence and numerical stability.\")\n",
    "print(\"*   **Tree-based algorithms (Decision Trees, Random Forests, XGBoost)**: Generally invariant to scaling as they rely on split points and relative order, not magnitude.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83efc406-73b7-4576-84f9-a9f2f094bf77",
   "metadata": {},
   "source": [
    "7. Save processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df798ebd-e6f7-40cf-beb6-675e7c3675b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved processed dataset to 'adult_processed.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convert the processed NumPy array X_processed back to a pandas DataFrame for saving with meaningful column names\n",
    "# Get feature names after one-hot encoding\n",
    "final_feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "processed_df = pd.DataFrame(\n",
    "    X_processed.toarray() if hasattr(X_processed, \"toarray\") else X_processed, \n",
    "    columns=final_feature_names\n",
    ")\n",
    "processed_df[target_feature] = y_encoded\n",
    "\n",
    "output_filename = 'adult_processed.csv'\n",
    "processed_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nSuccessfully saved processed dataset to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb2d0614-7d0a-4596-b550-076d8586a9d7",
   "metadata": {},
   "source": [
    "Key Changes:\n",
    "\n",
    "1. Standardized Scales: Numerical columns like age, fnlwgt, and hours-per-week will no longer be integers like 39 or 22635. They will be floats centered around 0.0 (standardized via StandardScaler).\n",
    "    \n",
    "2. Expanded Dimensionality: The number of columns will have increased from the original 15 to roughly 89–108 depending on how many unique values were in your categorical features.\n",
    "    \n",
    "3. One-Hot Binary Values: Categorical columns like workclass will have been replaced by multiple binary columns (e.g., workclass_Private, workclass_Self-emp-not-inc) containing only 0.0 or 1.0.\n",
    "    \n",
    "4. No Text (Strings): Every value in the DataFrame will be numerical (float or int). Strings like 'Bachelors', 'Male', or 'United-States' will have been completely converted.\n",
    "    \n",
    "5. Encoded Target: The income column will contain only 0 (for <=50K) and 1 (for >50K) if you applied LabelEncoder to the target. \n",
    "\n",
    "Modern machine learning pipelines require this feature parity where all inputs are on a comparable numerical scale. This prevents the model from being biased toward features with larger raw magnitudes (like capital-gain) and allows distance-based algorithms like SVM or KNN to function correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
